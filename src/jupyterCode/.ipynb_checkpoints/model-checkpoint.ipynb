{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "from collections import OrderedDict \n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionL(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(TransitionL, self).__init__()\n",
    "        self.transit = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_features),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(in_features, out_features, 1),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "    def forward(self, x):\n",
    "        return self.transit(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 128, 128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = TransitionL(1, 20)\n",
    "test1(torch.ones([1, 1, 256, 256])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper params\n",
    "B = 2 # Num bounding box in one grid cell\n",
    "S = 7 # Num gridcell\n",
    "C = 1 # Num classes\n",
    "IMG_SIZE = 448\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _DenseLayer(nn.Module):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                                           growth_rate, kernel_size=1, stride=1,\n",
    "                                           bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                                           kernel_size=3, stride=1, padding=1,\n",
    "                                           bias=False)),\n",
    "        self.drop_rate = float(drop_rate)\n",
    "        self.memory_efficient = memory_efficient\n",
    "\n",
    "    def bn_function(self, inputs):\n",
    "        \"Bottleneck function\"\n",
    "        # type: (List[Tensor]) -> Tensor\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))  # noqa: T484\n",
    "        return bottleneck_output\n",
    "\n",
    "    def forward(self, input):  \n",
    "        if isinstance(input, torch.Tensor):\n",
    "            prev_features = [input]\n",
    "        else:\n",
    "            prev_features = input\n",
    "\n",
    "        bottleneck_output = self.bn_function(prev_features)\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
    "                                     training=self.training)\n",
    "        return new_features\n",
    "    \n",
    "class _DenseBlock(nn.ModuleDict):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient,\n",
    "            )\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        for name, layer in self.items():\n",
    "            new_features = layer(features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0, memory_efficient=False):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        # Convolution and pooling part from table-1\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2,\n",
    "                                padding=3, bias=False)),\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "\n",
    "        # Add multiple denseblocks based on config \n",
    "        # for densenet-121 config: [6,12,24,16]\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                num_input_features=num_features,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient\n",
    "            )\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                # add transition layer between denseblocks to \n",
    "                # downsample\n",
    "                trans = TransitionL(num_features,\n",
    "                                    num_features // 2)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        self.lastconv = nn.Conv2d(num_features, num_features, 1, 2)\n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        features_map = self.lastconv(features)\n",
    "        return features_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones([1, 3, 448, 448])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 7, 7])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(YOLOD, self).__init__()\n",
    "        self.feature_extractor = DenseNet()\n",
    "        self.grid = S\n",
    "        self.num_classes = C\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(S*S*1024, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.Dropout(p=0.1), \n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Linear(4096, self.grid*self.grid*(self.num_classes + B*5))\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        flatten = torch.flatten(features)\n",
    "        print(flatten.size())\n",
    "        flatten = flatten.view(x.size()[0], -1)\n",
    "        print(flatten.size())\n",
    "\n",
    "        linear_vec = self.linear_layers(flatten)\n",
    "        output = linear_vec.view(-1, self.grid, self.grid, self.num_classes + B*5)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoloS = YOLOD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100352])\n",
      "torch.Size([2, 50176])\n"
     ]
    }
   ],
   "source": [
    "out = yoloS(torch.ones([2, 3, 448, 448]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (9408,) (64,) (9408,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-589556589249>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (9408,) (64,) (9408,) "
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for params in yoloS.named_parameters():\n",
    "    \n",
    "    l = params[1].detach().numpy().ravel()\n",
    "    total += l\n",
    "print(total)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = {\"train\": transforms.Compose([\n",
    "    transforms.Resize(448),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),]),\n",
    "               \"test\": transforms.Compose([\n",
    "    transforms.Resize(448),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"../../../yolo-pytorch/data/global-wheat-detection/train.csv\"\n",
    "\n",
    "image_link = \"../../../yolo-pytorch/data/global-wheat-detection/train\"\n",
    "\n",
    "class GlobalWheatData(Dataset):\n",
    "    def __init__(self, csv_file, image_link, preprocess, img_size = 448, mode = \"train\"):\n",
    "        \n",
    "        super(GlobalWheatData, self).__init__()\n",
    "        self.file = csv_file\n",
    "        self.img_size = img_size\n",
    "        self.wheat_size = 1024\n",
    "        self.image_link = image_link\n",
    "        self.mode = mode\n",
    "        self.preprocess = preprocess\n",
    "        self.data_x = []\n",
    "        self.data_y = []\n",
    "        self.load_data()\n",
    "    def load_data(self):\n",
    "        df = pd.read_csv(self.file)\n",
    "        box_coord = df[[\"image_id\", \"bbox\"]].groupby(\"image_id\")[\"bbox\"].apply(list).reset_index()\n",
    "        mapDict = {k:v for k, v in zip(box_coord[\"image_id\"], box_coord[\"bbox\"])}\n",
    "        N = len(mapDict.keys())\n",
    "        X = np.zeros((N, self.img_size, self.img_size, 3), dtype='uint8')\n",
    "        for idx, (id, boxes) in enumerate(mapDict.items()):\n",
    "            image_name = self.image_link + \"/\" + id + \".jpg\"\n",
    "            X = Image.open(image_name)\n",
    "            img_tensor = self.preprocess_img(X)            \n",
    "            y = np.zeros((S, S, 5+ C))\n",
    "            for i, box in enumerate(boxes):\n",
    "                box = json.loads(box)\n",
    "                xmin, ymin, w, h = box[0], box[1], box[2], box[3]\n",
    "                # convert coord from 1024 image size to 448 image size\n",
    "                xmin, ymin, w, h = xmin/self.wheat_size * 448, ymin/1024 * 448, w/1024 * 448, h/1024 * 448\n",
    "                x_center, y_center = (xmin+w)/2, (ymin+h)/2\n",
    "                x_idx, y_idx = int(x_center/self.img_size * S), int(y_center/self.img_size * S)\n",
    "                y[x_idx, y_idx] = 1, int(x_center), int(y_center), int(w), int(h), 1\n",
    "            \n",
    "            self.data_x.append(img_tensor)\n",
    "            self.data_y.append(y)\n",
    "            break\n",
    "    def preprocess_img(self, img):\n",
    "        if self.mode == \"train\":\n",
    "            return_img = self.preprocess[self.mode](img)\n",
    "        elif self.mode == \"test\":\n",
    "            return_img = self.preprocess[self.mode](img)\n",
    "        else:\n",
    "            raise Exception(\"Wrong mode\")\n",
    "        return return_img\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data_x[idx]\n",
    "        y = self.data_y[idx]\n",
    "        return X, y\n",
    "        \n",
    "\n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = GlobalWheatData(link, image_link, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = handler.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/son/yolo-pytorch/src/jupyterCode\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c42a15b2c7cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7954e21de2e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbox_coord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bbox\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"image_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bbox\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "box_coord = df[[\"image_id\", \"bbox\"]].groupby(\"image_id\")[\"bbox\"].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00333207f</td>\n",
       "      <td>[[0, 654, 37, 111], [0, 817, 135, 98], [0, 192...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005b0d8bb</td>\n",
       "      <td>[[765.0, 879.0, 116.0, 79.0], [84.0, 539.0, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006a994f7</td>\n",
       "      <td>[[437.0, 988.0, 98.0, 36.0], [309.0, 527.0, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00764ad5d</td>\n",
       "      <td>[[89.0, 256.0, 113.0, 107.0], [216.0, 282.0, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00b5fefed</td>\n",
       "      <td>[[709.0, 97.0, 204.0, 105.0], [775.0, 250.0, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id                                               bbox\n",
       "0  00333207f  [[0, 654, 37, 111], [0, 817, 135, 98], [0, 192...\n",
       "1  005b0d8bb  [[765.0, 879.0, 116.0, 79.0], [84.0, 539.0, 15...\n",
       "2  006a994f7  [[437.0, 988.0, 98.0, 36.0], [309.0, 527.0, 11...\n",
       "3  00764ad5d  [[89.0, 256.0, 113.0, 107.0], [216.0, 282.0, 1...\n",
       "4  00b5fefed  [[709.0, 97.0, 204.0, 105.0], [775.0, 250.0, 1..."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_coord.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapDict = {k:v for k, v in zip(box_coord[\"image_id\"], box_coord[\"bbox\"])}\n",
    "N = len(mapDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7, 6)\n"
     ]
    }
   ],
   "source": [
    "for idx, (id, boxes) in enumerate(mapDict.items()):\n",
    "    y = np.zeros((7, 7, 5+ 1))\n",
    "    for i, box in enumerate(boxes):\n",
    "        box = json.loads(box)\n",
    "        xmin, ymin, w, h = box[0], box[1], box[2], box[3]\n",
    "        # convert coord from 1024 image size to 448 image size\n",
    "        xmin, ymin, w, h = xmin/1024 * 448, ymin/1024 * 448, w/1024 * 448, h/1024 * 448\n",
    "        x_center, y_center = (xmin+w)/2, (ymin+h)/2\n",
    "        x_idx, y_idx = int(x_center/448 * S), int(y_center/448 * S)\n",
    "        y[x_idx, y_idx] = 1, int(x_center), int(y_center), int(w), int(h), 1\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "coo_mask = np.zeros((7, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "coo_mask[handler[0][1][..., 0] == 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True, False, False, False],\n",
       "       [ True,  True,  True,  True, False, False, False],\n",
       "       [ True,  True,  True,  True, False, False, False],\n",
       "       [ True,  True,  True,  True, False, False, False],\n",
       "       [False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler[0][1][..., 0] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 6)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coo_mask\n",
    "target = handler[0][1]\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-cdccac95450b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "target.unsqueeze(0).expand_as(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 7, 11])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classification loss\n",
    "out.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = handler[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.ones_like(torch.from_numpy(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True, False, False, False],\n",
       "       [ True,  True,  True,  True, False, False, False],\n",
       "       [ True,  True,  True,  True, False, False, False],\n",
       "       [ True,  True,  True,  True, False, False, False],\n",
       "       [False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[..., 0] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(294,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-52bcda4849d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 3"
     ]
    }
   ],
   "source": [
    "b[a[..., 0] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((7, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unsqueeze(-1).expand_as(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = a[..., 0] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
