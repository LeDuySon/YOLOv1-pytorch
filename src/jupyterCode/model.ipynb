{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "from collections import OrderedDict \n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionL(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(TransitionL, self).__init__()\n",
    "        self.transit = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_features),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(in_features, out_features, 1),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "    def forward(self, x):\n",
    "        return self.transit(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 128, 128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = TransitionL(1, 20)\n",
    "test1(torch.ones([1, 1, 256, 256])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper params\n",
    "B = 2 # Num bounding box in one grid cell\n",
    "S = 7 # Num gridcell\n",
    "C = 1 # Num classes\n",
    "IMG_SIZE = 448\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _DenseLayer(nn.Module):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                                           growth_rate, kernel_size=1, stride=1,\n",
    "                                           bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                                           kernel_size=3, stride=1, padding=1,\n",
    "                                           bias=False)),\n",
    "        self.drop_rate = float(drop_rate)\n",
    "        self.memory_efficient = memory_efficient\n",
    "\n",
    "    def bn_function(self, inputs):\n",
    "        \"Bottleneck function\"\n",
    "        # type: (List[Tensor]) -> Tensor\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))  # noqa: T484\n",
    "        return bottleneck_output\n",
    "\n",
    "    def forward(self, input):  \n",
    "        if isinstance(input, torch.Tensor):\n",
    "            prev_features = [input]\n",
    "        else:\n",
    "            prev_features = input\n",
    "\n",
    "        bottleneck_output = self.bn_function(prev_features)\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
    "                                     training=self.training)\n",
    "        return new_features\n",
    "    \n",
    "class _DenseBlock(nn.ModuleDict):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient,\n",
    "            )\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        for name, layer in self.items():\n",
    "            new_features = layer(features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0, memory_efficient=False):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        # Convolution and pooling part from table-1\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2,\n",
    "                                padding=3, bias=False)),\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "\n",
    "        # Add multiple denseblocks based on config \n",
    "        # for densenet-121 config: [6,12,24,16]\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                num_input_features=num_features,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient\n",
    "            )\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                # add transition layer between denseblocks to \n",
    "                # downsample\n",
    "                trans = TransitionL(num_features,\n",
    "                                    num_features // 2)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        self.lastconv = nn.Conv2d(num_features, num_features, 1, 2)\n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        features_map = self.lastconv(features)\n",
    "        return features_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones([1, 3, 448, 448])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 7, 7])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(YOLOD, self).__init__()\n",
    "        self.feature_extractor = DenseNet()\n",
    "        self.grid = S\n",
    "        self.num_classes = C\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(S*S*1024, 4096),\n",
    "            nn.Dropout(p=0.1), \n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Linear(4096, self.grid*self.grid*(self.num_classes + B*5))\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        flatten = torch.flatten(features)\n",
    "        print(flatten.size())\n",
    "        flatten = flatten.view(x.size()[0], -1)\n",
    "        print(flatten.size())\n",
    "\n",
    "        linear_vec = self.linear_layers(flatten)\n",
    "        output = linear_vec.view(-1, self.grid, self.grid, self.num_classes + B*5)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoloS = YOLOD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100352])\n",
      "torch.Size([2, 50176])\n"
     ]
    }
   ],
   "source": [
    "out = yoloS(torch.ones([2, 3, 448, 448]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (9408,) (64,) (9408,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-589556589249>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (9408,) (64,) (9408,) "
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for params in yoloS.named_parameters():\n",
    "    \n",
    "    l = params[1].detach().numpy().ravel()\n",
    "    total += l\n",
    "print(total)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = {\"train\": transforms.Compose([\n",
    "    transforms.Resize(448),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),]),\n",
    "               \"test\": transforms.Compose([\n",
    "    transforms.Resize(448),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"../../../yolo-pytorch/data/global-wheat-detection/train.csv\"\n",
    "\n",
    "image_link = \"../../../yolo-pytorch/data/global-wheat-detection/train\"\n",
    "\n",
    "class GlobalWheatData(Dataset):\n",
    "    def __init__(self, csv_file, image_link, preprocess, img_size = 448, mode = \"train\"):\n",
    "        \n",
    "        super(GlobalWheatData, self).__init__()\n",
    "        self.file = csv_file\n",
    "        self.img_size = img_size\n",
    "        self.wheat_size = 1024\n",
    "        self.image_link = image_link\n",
    "        self.mode = mode\n",
    "        self.preprocess = preprocess\n",
    "        self.data_x = []\n",
    "        self.data_y = []\n",
    "        self.load_data()\n",
    "    def load_data(self):\n",
    "        df = pd.read_csv(self.file)\n",
    "        box_coord = df[[\"image_id\", \"bbox\"]].groupby(\"image_id\")[\"bbox\"].apply(list).reset_index()\n",
    "        mapDict = {k:v for k, v in zip(box_coord[\"image_id\"], box_coord[\"bbox\"])}\n",
    "        N = len(mapDict.keys())\n",
    "        X = np.zeros((N, self.img_size, self.img_size, 3), dtype='uint8')\n",
    "        for idx, (id, boxes) in enumerate(mapDict.items()):\n",
    "            image_name = self.image_link + \"/\" + id + \".jpg\"\n",
    "            X = Image.open(image_name)\n",
    "            img_tensor = self.preprocess_img(X)            \n",
    "            y = np.zeros((S, S, 5*B+ C))\n",
    "            for i, box in enumerate(boxes):\n",
    "                box = json.loads(box)\n",
    "                xmin, ymin, w, h = box[0], box[1], box[2], box[3]\n",
    "                # convert coord from 1024 image size to 448 image size\n",
    "                xmin, ymin, w, h = xmin/self.wheat_size * 448, ymin/1024 * 448, w/1024 * 448, h/1024 * 448\n",
    "                x_center, y_center = (xmin+w)/2, (ymin+h)/2\n",
    "                x_idx, y_idx = int(x_center/self.img_size * S), int(y_center/self.img_size * S)\n",
    "                y[x_idx, y_idx] = 1, int(x_center), int(y_center), int(w), int(h), 1, int(x_center), int(y_center), int(w), int(h), 1\n",
    "            \n",
    "            self.data_x.append(img_tensor)\n",
    "            self.data_y.append(y)\n",
    "            break\n",
    "    def preprocess_img(self, img):\n",
    "        if self.mode == \"train\":\n",
    "            return_img = self.preprocess[self.mode](img)\n",
    "        elif self.mode == \"test\":\n",
    "            return_img = self.preprocess[self.mode](img)\n",
    "        else:\n",
    "            raise Exception(\"Wrong mode\")\n",
    "        return return_img\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data_x[idx]\n",
    "        y = self.data_y[idx]\n",
    "        return X, y\n",
    "        \n",
    "\n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = GlobalWheatData(link, image_link, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = handler.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = handler[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50176])\n",
      "torch.Size([1, 50176])\n"
     ]
    }
   ],
   "source": [
    "pred = yoloS(X.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 11)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.from_numpy(y).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 7, 11])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "coo_mask = y[..., 0] == 1\n",
    "noo_mask = y[..., 0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 7])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coo_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "coo_mask = coo_mask.unsqueeze(-1).expand_as(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 7, 11])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coo_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([176])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coo_pred_mask = pred[coo_mask]\n",
    "coo_pred_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "coo_pred_mask = coo_pred_mask.view(-1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 11])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noo_target_mask = pred[noo_mask]\n",
    "noo_target_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4782, -0.2865, -0.3194,  0.0363,  0.0979,  0.5161, -0.1517, -0.1302,\n",
       "          0.3840, -0.0491, -0.3200],\n",
       "        [ 0.2188,  0.1053, -0.1791, -0.1166,  0.3733, -0.5443, -0.0927,  0.1693,\n",
       "         -0.3498,  0.1636,  0.2443],\n",
       "        [ 0.0245,  0.1688,  0.2279, -0.3182,  0.0791,  0.3367, -0.2535,  0.2820,\n",
       "         -0.2257, -0.1069, -0.2360],\n",
       "        [ 0.3293,  0.1579, -0.2639,  0.1323, -0.2710,  0.4613, -0.1528,  0.3648,\n",
       "         -0.0500, -0.0994,  0.1221],\n",
       "        [-0.4594,  0.2573, -0.3525,  0.0715, -0.0285,  0.5301, -0.0372,  0.0631,\n",
       "         -0.4290, -0.1701, -0.4195],\n",
       "        [-0.6249, -0.4430, -0.1884,  0.2141,  0.0109, -0.1479, -0.3562, -0.0515,\n",
       "         -0.5866, -0.0300, -0.0970],\n",
       "        [ 0.2597, -0.2929,  0.1352,  0.4356, -0.1567, -0.0259, -0.2835, -0.5129,\n",
       "          0.2261, -0.6344,  0.2946],\n",
       "        [ 0.0536, -0.6549,  0.0992, -0.0057,  0.3611,  0.1571,  0.3575,  0.4274,\n",
       "          0.4644,  0.8024, -0.0475],\n",
       "        [-0.0253, -0.1476,  0.1739, -0.1054,  0.6220,  0.0961, -0.0349,  0.1445,\n",
       "          0.2255,  0.3911, -0.4274],\n",
       "        [ 0.2980, -0.3891, -0.2888, -0.8227, -0.2268,  0.1070,  0.2454, -0.0767,\n",
       "         -0.1086, -0.0563, -0.1654],\n",
       "        [-0.1007, -0.3478, -0.1634, -0.0729,  1.0934,  0.0632,  0.0150,  0.0136,\n",
       "          0.0593,  0.4024,  0.2072],\n",
       "        [-0.0128,  0.5094,  0.0253, -0.2483, -0.2384, -0.5169, -0.2489,  0.0165,\n",
       "          0.3165,  0.6430,  0.3091],\n",
       "        [ 0.3829, -0.0473, -0.0289,  0.1046, -0.2113,  0.1886, -0.1386, -0.0808,\n",
       "         -0.1438, -0.4990,  0.3373],\n",
       "        [ 0.0413, -0.1583,  0.1091,  0.4178, -0.2850,  0.0711, -0.1618,  0.4035,\n",
       "          0.0894,  0.0868, -0.1308],\n",
       "        [-0.1955,  0.5079, -0.4985, -0.3391,  0.3841,  0.0766, -0.0623,  0.0097,\n",
       "          0.1709,  0.0429, -0.1070],\n",
       "        [ 0.0802,  0.4727, -0.1483, -0.0541, -0.0660,  0.3242, -0.0154,  0.5495,\n",
       "          0.0166,  0.7939,  0.6780]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coo_pred_mask.view(-1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "coo_target_mask = y[coo_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1.,  20.,  43.,  38.,  25.,   1.,  20.,  43.,  38.,  25.,   1.],\n",
       "        [  1.,  53.,  80.,  30.,  23.,   1.,  53.,  80.,  30.,  23.,   1.],\n",
       "        [  1.,   8., 167.,  16.,  48.,   1.,   8., 167.,  16.,  48.,   1.],\n",
       "        [  1.,  29., 200.,  59.,  42.,   1.,  29., 200.,  59.,  42.,   1.],\n",
       "        [  1., 126.,  57.,  47.,  23.,   1., 126.,  57.,  47.,  23.,   1.],\n",
       "        [  1., 103., 120.,  22.,  21.,   1., 103., 120.,  22.,  21.,   1.],\n",
       "        [  1., 106., 154.,  30.,  31.,   1., 106., 154.,  30.,  31.,   1.],\n",
       "        [  1., 116., 209.,  27.,  26.,   1., 116., 209.,  27.,  26.,   1.],\n",
       "        [  1., 177.,  56.,  44.,  35.,   1., 177.,  56.,  44.,  35.,   1.],\n",
       "        [  1., 139., 104.,  38.,  32.,   1., 139., 104.,  38.,  32.,   1.],\n",
       "        [  1., 188., 129.,  70.,  38.,   1., 188., 129.,  70.,  38.,   1.],\n",
       "        [  1., 170., 211.,  76.,  39.,   1., 170., 211.,  76.,  39.,   1.],\n",
       "        [  1., 202.,  37.,  28.,  22.,   1., 202.,  37.,  28.,  22.,   1.],\n",
       "        [  1., 219., 104.,  22.,  24.,   1., 219., 104.,  22.,  24.,   1.],\n",
       "        [  1., 214., 171.,  36.,  22.,   1., 214., 171.,  36.,  22.,   1.],\n",
       "        [  1., 220., 219.,  67.,  67.,   1., 220., 219.,  67.,  67.,   1.]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coo_target_mask.view(-1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-35-05688ed7254c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-05688ed7254c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pred_noo_mask =\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pred_noo_mask = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c089687e87f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpred_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoo_pred_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtarget_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoo_target_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "pred_conf = coo_pred_mask[:, 0]\n",
    "target_conf = coo_target_mask[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.zeros_like(torch.ones(16, 11))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[..., 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-612806932de2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcoo_pred_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "coo_pred_mask[torch.BoolTensor(test == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4782, -0.2865, -0.3194,  0.0363,  0.0979,  0.5161, -0.1517, -0.1302,\n",
       "          0.3840, -0.0491, -0.3200],\n",
       "        [ 0.2188,  0.1053, -0.1791, -0.1166,  0.3733, -0.5443, -0.0927,  0.1693,\n",
       "         -0.3498,  0.1636,  0.2443],\n",
       "        [ 0.0245,  0.1688,  0.2279, -0.3182,  0.0791,  0.3367, -0.2535,  0.2820,\n",
       "         -0.2257, -0.1069, -0.2360],\n",
       "        [ 0.3293,  0.1579, -0.2639,  0.1323, -0.2710,  0.4613, -0.1528,  0.3648,\n",
       "         -0.0500, -0.0994,  0.1221],\n",
       "        [-0.4594,  0.2573, -0.3525,  0.0715, -0.0285,  0.5301, -0.0372,  0.0631,\n",
       "         -0.4290, -0.1701, -0.4195],\n",
       "        [-0.6249, -0.4430, -0.1884,  0.2141,  0.0109, -0.1479, -0.3562, -0.0515,\n",
       "         -0.5866, -0.0300, -0.0970],\n",
       "        [ 0.2597, -0.2929,  0.1352,  0.4356, -0.1567, -0.0259, -0.2835, -0.5129,\n",
       "          0.2261, -0.6344,  0.2946],\n",
       "        [ 0.0536, -0.6549,  0.0992, -0.0057,  0.3611,  0.1571,  0.3575,  0.4274,\n",
       "          0.4644,  0.8024, -0.0475],\n",
       "        [-0.0253, -0.1476,  0.1739, -0.1054,  0.6220,  0.0961, -0.0349,  0.1445,\n",
       "          0.2255,  0.3911, -0.4274],\n",
       "        [ 0.2980, -0.3891, -0.2888, -0.8227, -0.2268,  0.1070,  0.2454, -0.0767,\n",
       "         -0.1086, -0.0563, -0.1654],\n",
       "        [-0.1007, -0.3478, -0.1634, -0.0729,  1.0934,  0.0632,  0.0150,  0.0136,\n",
       "          0.0593,  0.4024,  0.2072],\n",
       "        [-0.0128,  0.5094,  0.0253, -0.2483, -0.2384, -0.5169, -0.2489,  0.0165,\n",
       "          0.3165,  0.6430,  0.3091],\n",
       "        [ 0.3829, -0.0473, -0.0289,  0.1046, -0.2113,  0.1886, -0.1386, -0.0808,\n",
       "         -0.1438, -0.4990,  0.3373],\n",
       "        [ 0.0413, -0.1583,  0.1091,  0.4178, -0.2850,  0.0711, -0.1618,  0.4035,\n",
       "          0.0894,  0.0868, -0.1308],\n",
       "        [-0.1955,  0.5079, -0.4985, -0.3391,  0.3841,  0.0766, -0.0623,  0.0097,\n",
       "          0.1709,  0.0429, -0.1070],\n",
       "        [ 0.0802,  0.4727, -0.1483, -0.0541, -0.0660,  0.3242, -0.0154,  0.5495,\n",
       "          0.0166,  0.7939,  0.6780]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coo_pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.cat((noo_target_mask[..., 0:4], noo_target_mask[..., 5:8]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.ones_like(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 7])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 7])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "def a(*s):\n",
    "    a, b, c = s\n",
    "    print(a, b ,c)\n",
    "a(1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
